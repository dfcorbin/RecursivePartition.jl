var documenterSearchIndex = {"docs":
[{"location":"pcb/#Polynomial-Chaos-Basis","page":"Polynomial Chaos Basis","title":"Polynomial Chaos Basis","text":"","category":"section"},{"location":"pcb/#Definition","page":"Polynomial Chaos Basis","title":"Definition","text":"","category":"section"},{"location":"pcb/","page":"Polynomial Chaos Basis","title":"Polynomial Chaos Basis","text":"The Polynomial Chaos Basis (PCB) is a natural way of extending univariate polynomial bases to the multivariate setting. I will provide a brief introduction to the PCB and how it is constructed, but I will make no attempt to motivate its applications. Upon completion of this documentation I will reference a more detailed summary of PCB applications and theoretical properties.","category":"page"},{"location":"pcb/","page":"Polynomial Chaos Basis","title":"Polynomial Chaos Basis","text":"Let ϕ_j(x) denote a univariate polynomial of degree j in mathbbN_ 0  e.g. ϕ_2(x) = x^2 using the Power Basis. We use this notation as there are other univariate bases to choose from, some of which possess useful theoretical properties like orthogonality (see Legendre Polynomials). One way of defining a multivariate polynomial (MVP) is to take a multiplicative combination of univariate polynomials. This can be indexed using a vector of non-negative integers as follows. Let boldsymbolα = 1 3 5, the corresponding 3-dimensional multivariate polynomial is given by","category":"page"},{"location":"pcb/","page":"Polynomial Chaos Basis","title":"Polynomial Chaos Basis","text":"  psi_boldsymbolα(boldsymbolx) = ϕ_1(x_1) cdot ϕ_3(x_2) cdot ϕ_5(x_3)","category":"page"},{"location":"pcb/","page":"Polynomial Chaos Basis","title":"Polynomial Chaos Basis","text":"The d-dimensional PCB is then defined as the collection of all MVPs which can be indexed in this way.","category":"page"},{"location":"pcb/","page":"Polynomial Chaos Basis","title":"Polynomial Chaos Basis","text":"  lbrace ψ_boldsymbolα quad  quad boldsymbolα in mathbbN_ 0^d rbrace","category":"page"},{"location":"pcb/","page":"Polynomial Chaos Basis","title":"Polynomial Chaos Basis","text":"However, in practice this basis must be truncated in order to be used. This can be achieved by restricting the total degree (defined as the sum of the elements in boldsymbolα)) of the MVPs.","category":"page"},{"location":"pcb/","page":"Polynomial Chaos Basis","title":"Polynomial Chaos Basis","text":"  lbrace ψ_boldsymbolalpha quad  quad boldsymbolalpha_1 leqq J\n  text  boldsymbolalpha in mathbbN_geqq 0^d rbrace","category":"page"},{"location":"pcb/","page":"Polynomial Chaos Basis","title":"Polynomial Chaos Basis","text":"Now suppose you have an n times d matrix boldsymbolX of real numbers. Each row of boldsymbolX is a vector on which we can evaluate a MVP. We can therefore construct a new matrix where the entry in the i'th row and j'th column is the i'th row of boldsymbolX evaluated at the j'th basis function in our PCB i.e. each column of the transformed matrix corresponds to a different MVP in the truncated PCB. This matrix is what we refer to as the \"PCB expansion\" of boldsymbolX.","category":"page"},{"location":"pcb/","page":"Polynomial Chaos Basis","title":"Polynomial Chaos Basis","text":"boldsymbolΨ_ij = ψ_boldsymbolα_j(boldsymbolx_i)","category":"page"},{"location":"pcb/#Implementation-Details","page":"Polynomial Chaos Basis","title":"Implementation Details","text":"","category":"section"},{"location":"pcb/","page":"Polynomial Chaos Basis","title":"Polynomial Chaos Basis","text":"Currently, I have only implemented the Legendre Polynomials variant of the PCB. Since the Legendre Polynomials are only defined on -1 1, the user must provide upper/lower bounds for each column of boldsymbolX (it is assumed that each column is uniformly distributed between these bounds). These bounds are used to rescale the columns so that they are all uniformly distributed on -1 1. Bounds are provided as a d times 2 matrix. For example, if d = 2, x_1 in 0 1 and x_2 in 1 2, the bounds are represented as","category":"page"},{"location":"pcb/","page":"Polynomial Chaos Basis","title":"Polynomial Chaos Basis","text":"beginpmatrix\n  0  1 \n  1  2\nendpmatrix","category":"page"},{"location":"pcb/","page":"Polynomial Chaos Basis","title":"Polynomial Chaos Basis","text":"Constructing the PCB expansion of a matrix using RecursivePartition.trunc_pcbmat is then straightforward.","category":"page"},{"location":"pcb/","page":"Polynomial Chaos Basis","title":"Polynomial Chaos Basis","text":"using RecursivePartition\n\nX = [0.0 0.5; -0.5 0.0]\nkmat = repeat([-1.0 1.0], 2, 1)\n\ntrunc_pcbmat(X, 2, kmat)\n\n# output\n\n2×5 Array{Float64,2}:\n 0.5  -0.125   0.0   0.0  -0.5\n 0.0  -0.5    -0.5  -0.0  -0.125","category":"page"},{"location":"pcb/","page":"Polynomial Chaos Basis","title":"Polynomial Chaos Basis","text":"If there are specific MVPs you wish to use in the basis expansion (as opposed to the full truncated PCB), this is achieved using RecursivePartition.index_pcbmat. This function accepts a vector of RecursivePartition.MVPIndex objects which correspond to your choice of MVPs. To construct such an object, we supply two vectors of even length, the first gives the degree of each multiplicative term in the MVP, the second gives the covariate it applies to. For example, to store the MVP","category":"page"},{"location":"pcb/","page":"Polynomial Chaos Basis","title":"Polynomial Chaos Basis","text":"ϕ_1(x_1)  ϕ_3(x_2)","category":"page"},{"location":"pcb/","page":"Polynomial Chaos Basis","title":"Polynomial Chaos Basis","text":"we create the object","category":"page"},{"location":"pcb/","page":"Polynomial Chaos Basis","title":"Polynomial Chaos Basis","text":"MVPIndex([1, 3], [1, 2])","category":"page"},{"location":"pcb/","page":"Polynomial Chaos Basis","title":"Polynomial Chaos Basis","text":"This may seem like an odd method for storing an MVP, but it turns out to be far more memory efficient than other methods in higher dimensions. As an alternative to manually constructing RecursivePartition.MVPIndex objects, you can generate the full truncated PCB basis using RecursivePartition.mvpindex and select the basis functions you wish to use.","category":"page"},{"location":"pcb/","page":"Polynomial Chaos Basis","title":"Polynomial Chaos Basis","text":"mvpindex(2, 2) # 2D PCB basis trunacted with J = 2\n\n# output\n\n5-element Array{MVPIndex,1}:\n MVPIndex([1], [2])\n MVPIndex([2], [2])\n MVPIndex([1], [1])\n MVPIndex([1, 1], [1, 2])\n MVPIndex([2], [1])","category":"page"},{"location":"pcb/#Functions","page":"Polynomial Chaos Basis","title":"Functions","text":"","category":"section"},{"location":"pcb/","page":"Polynomial Chaos Basis","title":"Polynomial Chaos Basis","text":"Modules = [RecursivePartition]\nPages = [\"pcb.jl\"]","category":"page"},{"location":"pcb/#RecursivePartition.MVPIndex","page":"Polynomial Chaos Basis","title":"RecursivePartition.MVPIndex","text":"MVPIndex(deg::Vector{Int64}, dim::Vector{Int64})\n\nCompact structure used to index a multivariate polynomial (MVP).\n\nIt is assumed that this type is primarily constructed using mvpindex and supplied to index_pcbmat to perform a polynomial basis expansion.\n\ndeg gives the degree of each multiplicative term in the MVP.\ndim gives the covariate associated with each degree.\n\nSee also: mvpindex, index_pcbmat\n\nExamples\n\nLet's suppose we have the vector boldsymbolx = x_1 x_2 and we wish to index the multivariate polynomial ϕ_2(x_1) cdot ϕ_2(x)^3. This is achieved using the following object.\n\nMVPIndex([2, 3], [1, 2])\n\nNote that ϕ_j denotes a generic order j univariate polynomial.\n\n\n\n\n\n","category":"type"},{"location":"pcb/#RecursivePartition.index_pcbmat-Tuple{Array{Float64,2},Array{MVPIndex,1},Array{Float64,2}}","page":"Polynomial Chaos Basis","title":"RecursivePartition.index_pcbmat","text":"index_pcbmat(X::Matrix{Float64}, indices::Vector{MVPIndex}, kmat::Matrix{Float64})\n\nCompute the Polynomial Chaos Basis (PCB) expansion for each row of X for specified indices (generated using mvpindex).\n\nLegendre Polynomials are used as basis functions, hence an additional argument kmat is supplied to rescale the columns of X back on to -1 1.\n\nSee also: mvpindex, MVPIndex, trunc_pcbmat\n\nExamples\n\nX = [-0.5 0.5; 0.1 0.2]\nkmat = repeat([-1.0 1.0], 2, 1)\nind = mvpindex(2, 2)\n\nindex_pcbmat(X, ind, kmat)\n\n# output\n\n2×5 Array{Float64,2}:\n 0.5  -0.125  -0.5  -0.25  -0.125\n 0.2  -0.44    0.1   0.02  -0.485\n\n\n\n\n\n","category":"method"},{"location":"pcb/#RecursivePartition.mvpindex-Tuple{Int64,Int64}","page":"Polynomial Chaos Basis","title":"RecursivePartition.mvpindex","text":"mvpindex(dim::Int64, degmax::Int64)\n\nGenerate a vector containing all possible MVPIndex objects subject to a bound, degmax, on the total degree of each multivariate polynomial.\n\nThe total degree of a multivariate polynomial is defined as the sum of each individual multiplicative term's degree i.e. textdeg(x_1^2 x_2^3) = 2 + 3 = 5\n\nSee also: index_pcbmat\n\nExamples\n\n# Output all 2-dimensional MVPs with total degree less than 2.\n\nmvpindex(2, 2)\n\n# output\n\n5-element Array{MVPIndex,1}:\n MVPIndex([1], [2])\n MVPIndex([2], [2])\n MVPIndex([1], [1])\n MVPIndex([1, 1], [1, 2])\n MVPIndex([2], [1])\n\n\n\n\n\n","category":"method"},{"location":"pcb/#RecursivePartition.trunc_pcbmat-Tuple{Array{Float64,2},Int64,Array{Float64,2}}","page":"Polynomial Chaos Basis","title":"RecursivePartition.trunc_pcbmat","text":"trunc_pcbmat(X::Matrix{Float64}, degmax::Int64, kmat::Matrix{Float64})\n\nCompute the Polynomial Chaos Basis (PCB) using the full truncated PCB basis.\n\nLegendre Polynomials are used as basis functions, hence an additional argument kmat is supplied to rescale the columns of X back on to -1 1.\n\nSee also: mvpindex, MVPIndex, index_pcbmat\n\nExamples\n\nX = [-0.5 0.5; 0.1 0.2]\nkmat = repeat([-1.0 1.0], 2, 1)\n\ntrunc_pcbmat(X, 2, kmat)\n\n# output\n\n2×5 Array{Float64,2}:\n 0.5  -0.125  -0.5  -0.25  -0.125\n 0.2  -0.44    0.1   0.02  -0.485\n\n\n\n\n\n","category":"method"},{"location":"partition/#Recursive-Partitioning","page":"Recursive Partitioning","title":"Recursive Partitioning","text":"","category":"section"},{"location":"partition/","page":"Recursive Partitioning","title":"Recursive Partitioning","text":"The RecursivePartition module allows the user to partition data into (hyper-)rectangular subsets across a bounded space. I will start with a simple motivating example which demonstrates the partitioning functionality of this module. Suppose that you have a dataset containing 1000 pairs of coordinates which exist between 0 and 1 i.e. all of the points are positioned somewhere within the unit square. This information can be encoded using a matrix, where the i'th row details the upper/lower bounds of the i'th coordinate. Our square is therefore represented by the matrix","category":"page"},{"location":"partition/","page":"Recursive Partitioning","title":"Recursive Partitioning","text":"beginpmatrix\n  0  1 \n  0  1 \nendpmatrix","category":"page"},{"location":"partition/","page":"Recursive Partitioning","title":"Recursive Partitioning","text":"One way to approach dividing up this space is to simply split the square in half and determine which side each of the points falls on. Assuming we are splitting in dimension 1 (or the x-direction in terms of coordinates), this would look something like this:","category":"page"},{"location":"partition/","page":"Recursive Partitioning","title":"Recursive Partitioning","text":"(Image: Partition Demo 1)","category":"page"},{"location":"partition/","page":"Recursive Partitioning","title":"Recursive Partitioning","text":"We can now represent this as a vector of matrices, each matrix corresponding to a separate subregion.","category":"page"},{"location":"partition/","page":"Recursive Partitioning","title":"Recursive Partitioning","text":"P = left\nbeginpmatrix\n  0  05 \n  0  1\nendpmatrix\nbeginpmatrix\n  05  0 \n  0    1\nendpmatrix\nright","category":"page"},{"location":"partition/","page":"Recursive Partitioning","title":"Recursive Partitioning","text":"We can continue to recursively partition the resulting subsets into smaller rectangles e.g.","category":"page"},{"location":"partition/","page":"Recursive Partitioning","title":"Recursive Partitioning","text":"(Image: Partition Demo 2)","category":"page"},{"location":"partition/","page":"Recursive Partitioning","title":"Recursive Partitioning","text":"Partitions defined in this way can be constructed using RecursivePartition.insert_knot.","category":"page"},{"location":"partition/","page":"Recursive Partitioning","title":"Recursive Partitioning","text":"using RecursivePartition\n\nX = rand(1000, 2)\nP = [repeat([0.0 1.0], 2, 1)]\nP = insert_knot(P, 1, 1, 0.5) # Split in dimension 1.\nP = insert_knot(P, 1, 2, 0.5) # Split subregion 1 in dimension 2.\nXsubsets = partition(X, P) # Partition X according to P.","category":"page"},{"location":"partition/","page":"Recursive Partitioning","title":"Recursive Partitioning","text":"The output Xsubsets is a vector of matrices; each matrix contains the rows of X pertaining to a particular subset. Although it is convenient to visualize this process in 2D, it is easily extended to higher dimensions.","category":"page"},{"location":"partition/#Functions","page":"Recursive Partitioning","title":"Functions","text":"","category":"section"},{"location":"partition/","page":"Recursive Partitioning","title":"Recursive Partitioning","text":"Modules = [RecursivePartition]\nPages = [\"partition.jl\"]","category":"page"},{"location":"partition/#RecursivePartition.insert_knot!-Tuple{Array{Array{Float64,2},1},Int64,Int64,Float64}","page":"Recursive Partitioning","title":"RecursivePartition.insert_knot!","text":"insert_knot!(P::Matvec{Float64}, k::Int64, dim::Int64, loc::Float64)\n\nDo insert_knot by overwriting the original array (memory efficient).\n\n\n\n\n\n","category":"method"},{"location":"partition/#RecursivePartition.insert_knot-Tuple{Array{Array{Float64,2},1},Int64,Int64,Float64}","page":"Recursive Partitioning","title":"RecursivePartition.insert_knot","text":"insert_knot(P::Matvec{Float64}, k::Int64, dim::Int64, loc::Float64)\n\nBisect the k'th subset along dimension dim.\n\nThis function returns a modified version of P, where the k'th subset is replaced by the two matrices outputted by splitmat. The left matrix of the split is inserted into the k'th position, the right side is appended to the end.\n\nSee also: splitmat, insert_knot!\n\nExamples\n\nP = [repeat([-1.0 1.0], 2, 1)]\nP1 = insert_knot(P, 1, 1, 0.0)\n\n# output\n\n2-element Array{Array{Float64,2},1}:\n [-1.0 0.0; -1.0 1.0]\n [0.0 1.0; -1.0 1.0]\n\n\n\n\n\n","category":"method"},{"location":"partition/#RecursivePartition.partition-Tuple{Array{Float64,2},Array{Array{Float64,2},1},Array{Float64,1},Array{Float64,1}}","page":"Recursive Partitioning","title":"RecursivePartition.partition","text":"partition(X, P, [, y]; track=false)\n\nPartition a data matrix X (and optional vector y) into subsets according to a partition P.\n\nSee also: which_subset\n\nExamples\n\nP = [repeat([-1.0 1.0], 2, 1)]\nP = insert_knot!(P, 1, 1, 0.0)\nX = [-0.5 0.0; 0.5 0.0]\ny = [1.0, 2.0]\npartition(X, P, y; track=true)\n\n# output\n\n([[-0.5 0.0], [0.5 0.0]], [[1.0], [2.0]], [[1], [2]])\n\n\n\n\n\n","category":"method"},{"location":"partition/#RecursivePartition.splitmat-Tuple{Array{Float64,2},Int64,Float64}","page":"Recursive Partitioning","title":"RecursivePartition.splitmat","text":"splitmat(mat::Matrix{Float64}, dim::Int64, loc::Float64)\n\nReturn two new matrices that result from splitting row dim at loc.\n\nmat must have exactly two columns, with column two being strictly greater than column 1. loc must be contained by the row to be split.\n\n\n\n\n\n","category":"method"},{"location":"partition/#RecursivePartition.which_subset-Tuple{Array{Float64,1},Array{Array{Float64,2},1}}","page":"Recursive Partitioning","title":"RecursivePartition.which_subset","text":"which_subset(x::Vector{Float64}, P::Matvec{Float64})\n\nDetermine which subset a vector x is contained by. This function assumes that your partition P is disjoint and comprises the entire space under union.\n\nExamples\n\nP = [repeat([-1.0 1.0], 2, 1)]\ninsert_knot!(P, 1, 1, 0.0) # Create two daughter subsets by dividing dim 1.\nx = [-0.5, 0.0] # Dim 1 is les than 0.\n\nwhich_subset(x, P)\n\n# output\n\n1\n\n\n\n\n\n","category":"method"},{"location":"legendre/#Legendre-Polynomials","page":"Legendre Polynomials","title":"Legendre Polynomials","text":"","category":"section"},{"location":"legendre/#Tutorial","page":"Legendre Polynomials","title":"Tutorial","text":"","category":"section"},{"location":"legendre/","page":"Legendre Polynomials","title":"Legendre Polynomials","text":"The Legendre Polynomials are a set of univariate polynomials defined on the interval -1 1. Let \nϕ_j(x)  -1 1  mathbbR denote the order j in mathbbN_0 Legendre Polynomial. The Legendre Polynomials are defined by the property","category":"page"},{"location":"legendre/","page":"Legendre Polynomials","title":"Legendre Polynomials","text":"_-1^1 ϕ_i(x) ϕ_j(x) dx = 0 quad textfor all  i  j quad ϕ_0(x) = 1","category":"page"},{"location":"legendre/","page":"Legendre Polynomials","title":"Legendre Polynomials","text":"This property is referred to orthogonality. The theoretical benefits of orthogonality are beyond the scope of this documentation. For more detailed information about the theoretical properties of the Legendre Polynomials, see here. It turns out that Legendre polynomials can be defined by the second order recursive equation","category":"page"},{"location":"legendre/","page":"Legendre Polynomials","title":"Legendre Polynomials","text":"(n + 1) ϕ_n + 1(x) = (2 n + 1) x ϕ_n(x) - n ϕ_n - 1(x)","category":"page"},{"location":"legendre/","page":"Legendre Polynomials","title":"Legendre Polynomials","text":"We prodive two simple methods for evaluating Legendre Polynomials. The simplest approach is by using RecursivePartition.legendre_poly.","category":"page"},{"location":"legendre/","page":"Legendre Polynomials","title":"Legendre Polynomials","text":"using RecursivePartition\n\nlegendre_poly(5, 0.5) # 5th order Legendre Polynomial evaluated as x = 0.5\n\n# output\n\n0.08984375","category":"page"},{"location":"legendre/","page":"Legendre Polynomials","title":"Legendre Polynomials","text":"However, often the user wants to evaluate the Legendre Polynomials for a range of values e.g. j = 0 ldots 5. In this case, I recommend using the recursive definintion (RecursivePartition.legendre_next) directly.","category":"page"},{"location":"legendre/","page":"Legendre Polynomials","title":"Legendre Polynomials","text":"lp = Vector{Float64}(undef, 6)\nlp[1:2] = [1.0, 0.5]  # Must ϕ_0(x) and ϕ_1(x) as a start.\n\nfor j in 3:6 # Remember we are starting from order 0.\n  lp[j] = legendre_next(j - 1, 0.5, lp[j - 1], lp[j - 2])\nend\n\nlp\n\n# output\n\n6-element Array{Float64,1}:\n  1.0\n  0.5\n -0.125\n -0.4375\n -0.2890625\n  0.08984375","category":"page"},{"location":"legendre/#Functions","page":"Legendre Polynomials","title":"Functions","text":"","category":"section"},{"location":"legendre/","page":"Legendre Polynomials","title":"Legendre Polynomials","text":"Modules = [RecursivePartition]\nPages = [\"legendre.jl\"]","category":"page"},{"location":"legendre/#RecursivePartition.legendre_next-Tuple{Int64,Float64,Float64,Float64}","page":"Legendre Polynomials","title":"RecursivePartition.legendre_next","text":"legendre_next(ord, x, d1, d0)\n\nRecursive definition of the Legendre polynomials.\n\nThe Legendre Polynomials can be expressed as second order recursive equation, thus computing an order n  2 Legendre Polynomial is possible if we know the corresponding order n - 1 and n - 2 evaluations. These are supplied in the arguments d1 and d0 (note that the order 1 and 0 evaluations are always equal to x and 1.0 respectively).\n\nSee also: legendre_poly\n\nExamples\n\nord = 2\nx = 1.0\nd1 = 1.0\nd0 = 1.0\n\nlegendre_next(ord, x, d1, d0)\n\n# output\n\n1.0\n\n\n\n\n\n","category":"method"},{"location":"legendre/#RecursivePartition.legendre_poly-Tuple{Int64,Float64}","page":"Legendre Polynomials","title":"RecursivePartition.legendre_poly","text":"legendre_poly(ord, x)\n\nReturn the Legendre polynomial of specified order (ord) evaluated at x.\n\nSee also: legendre_next\n\nExamples\n\nlegendre_poly(1, 1.0)\n\n# output\n\n1.0\n\n\n\n\n\n","category":"method"},{"location":"regression/#Regression","page":"Regression","title":"Regression","text":"","category":"section"},{"location":"regression/","page":"Regression","title":"Regression","text":"The objective of regression is to model the relationship between a set of predictor and response variables. Once trained, the regression model can be used to predict the response variable, given a new predictor variable. RecursivePartition implements a variety of regression models, however its most notable feature is the PartitionModel type. This model is fitted by partitioning the space of predictor variables (see Recursive Partitioning for a visualization of this process) and fitting a flexible model within each subregion. One can think of this as a divide and conquer approach to learning complex relationships between predictors and responses.","category":"page"},{"location":"regression/","page":"Regression","title":"Regression","text":"Standard linear models will first be introduced, primarily motivating the use of the Polynomial Chaos Basis expansion as a method for incorporating a non-linear structure into regression models. The Partitioned Models tutorial then shows how we can aggregate the linear models across a partitioned space.","category":"page"},{"location":"regression/","page":"Regression","title":"Regression","text":"Note that the examples in this tutorial are at most 2-dimensional for visualization purposes. However, usage is identical for higher dimensional data.","category":"page"},{"location":"regression/#Bayesian-Linear-Models","page":"Regression","title":"Bayesian Linear Models","text":"","category":"section"},{"location":"regression/","page":"Regression","title":"Regression","text":"All the Bayesian linear models implemented in RecursivePartition currently assume that the predictor variables are Normally distributed with a mean given by some unknown function i.e.","category":"page"},{"location":"regression/","page":"Regression","title":"Regression","text":"y = f(boldsymbolx) + ϵ quad ϵ sim mathcalN(0 σ^2)","category":"page"},{"location":"regression/","page":"Regression","title":"Regression","text":"For this tutorial we choose an arbitrarily choose the mean function","category":"page"},{"location":"regression/","page":"Regression","title":"Regression","text":"f(x) = 2 x textsin(2 π x) - 2 x^2","category":"page"},{"location":"regression/","page":"Regression","title":"Regression","text":"and generate a random data set.","category":"page"},{"location":"regression/","page":"Regression","title":"Regression","text":"using RecursivePartition, Plots, Random\n\nRandom.seed!(100) # Set seed for reproducability\n\nN = 10000\nSD = 0.5\nf(x) = 2 * sin(2 * π * x[1]) * x[1] - 2 * x[1]^2\nx, y = gendat(N, SD, f, 1; bounds=[-1.0, 1.0])\n\nplot(x, y, seriestype=:scatter, alpha=0.1, label=\"data\")\nplot!(f, xlims=[-1.0, 1.0], lw=2, lty=2, s=:dash, label=\"f(x)\")","category":"page"},{"location":"regression/","page":"Regression","title":"Regression","text":"The figure below shows the plot outputted by the above code.","category":"page"},{"location":"regression/","page":"Regression","title":"Regression","text":"(Image: f plot)","category":"page"},{"location":"regression/","page":"Regression","title":"Regression","text":"Our job us thus to use the noisy blue data points to approximated the true mean of the data (given by the dashed red line). It is important to remember that the mean function f is used only to generate the data; it is unknown to the regression models.","category":"page"},{"location":"regression/","page":"Regression","title":"Regression","text":"In this tutorial we will fit three different Bayesian models to the dataset shown in the figure above.","category":"page"},{"location":"regression/","page":"Regression","title":"Regression","text":"A linear model.\nAn order 5 polynomial regression.\nAn order 10 polynomial regression.","category":"page"},{"location":"regression/","page":"Regression","title":"Regression","text":"In a polynomial regression we take the original predictor variables and transform them using a polynomial basis (see Polynomial Chaos Basis). Simply put, we are constructing the \"best\" polynomial which approximates f.","category":"page"},{"location":"regression/","page":"Regression","title":"Regression","text":"# Fit a linear model\nlinearmod = BayesLinearModel(x, y)\nlinfun = predfun(linearmod)\n\n# For plotting purposes, we need a function which accepts a float, not an\n# array...\nlinfun_flat(x) = linfun([x])\n\n# Fit an order 5 and 10 polynomial model\npolymod5 = PolyBLM(x, y, 5, [-1.0, 1.0])\npolymod10 = PolyBLM(x, y, 10, [-1.0, 1.0])\npolyfun5 = predfun(polymod5)\npolyfun10 = predfun(polymod10)\npolyfun5_flat(x) = polyfun5([x])\npolyfun10_flat(x) = polyfun10([x])\n\n# Create plot\nplot(linfun1, xlims=[-1.0, 1.0])\nplot!(f, xlims=[-1.0, 1.0], label=\"f(x)\", lw=3, s=:dash)\nplot!(linfun_flat, xlims=[-1.0, 1.0], label=\"Linear mod\")\nplot!(polyfun5_flat, xlims=[-1.0, 1.0], label=\"Deg 5 poly\")\nplot!(polyfun10_flat, xlims=[-1.0, 1.0], label=\"Deg 10 poly\")","category":"page"},{"location":"regression/","page":"Regression","title":"Regression","text":"In the above code, [predfun] outputs a function which accepts a vector argument. This function takes a predictor and ouputs the predicted response according to the model. The figure below shows the plot outputted using this code.","category":"page"},{"location":"regression/","page":"Regression","title":"Regression","text":"(Image: model plot)","category":"page"},{"location":"regression/","page":"Regression","title":"Regression","text":"Firstly we note that (unsurprisingly) the linear model performs very poorly here. This is because using BayesLinearModel restricts our model to a (linear) straight line, however the structure of the data is clearly non-linear. The order 5 polynomial model does a reasonable job of approximating f, however our model is still not flexible enough. Once we increase the order (and hence the complexity of the model) to 10, we finally have a model which accurately approximates f.","category":"page"},{"location":"regression/","page":"Regression","title":"Regression","text":"With the model trained, we can predict response variables as follows.","category":"page"},{"location":"regression/","page":"Regression","title":"Regression","text":"Xtest, ytest = gendat(N, SD, f, 1; bounds=[-1.0, 1.0])\nytest_approx = predict(polymod10, Xtest)","category":"page"},{"location":"regression/#Partitioned-Models","page":"Regression","title":"Partitioned Models","text":"","category":"section"},{"location":"regression/","page":"Regression","title":"Regression","text":"Although the polynomial regression models above may seem appealing, they quickly become impractical as the dimension of the predictor variables (and the degree of the model) increases. As an alternative approach, we can partition the space into separate regions and fit low order polynomial model within each one. For example, the following code will divide the data in half at 0, then fit an order 3 polynomial regression inside each half.","category":"page"},{"location":"regression/","page":"Regression","title":"Regression","text":"# Construct partition\nP = [[-1.0 1.0]] # Define the whole space (x lies between -1 and 1).\ninsert_knot!(P, 1, 1, 0.0) # Split in the middle.\n\n# Fit partitioned polynomial models\npartition_model = partition_polyblm(x, y, P; degmax=3)","category":"page"},{"location":"regression/","page":"Regression","title":"Regression","text":"We no longer provide code for producing figures, as the process is essentially the same. The figure below should the partitioned approximation","category":"page"},{"location":"regression/","page":"Regression","title":"Regression","text":"(Image: Static partition)","category":"page"},{"location":"regression/","page":"Regression","title":"Regression","text":"This approximation isn't terrible, but the natural response is to wonder whether we should partition the space further. In higher dimensional settings we don't have the luxury of plotting the function and choosing a sensible partition. Luckily, RecursivePartition implements a novel algorithm for dividing up the space automatically.","category":"page"},{"location":"regression/","page":"Regression","title":"Regression","text":"automod = auto_partition_polyblm(x, y, [-1.0, 1.0])","category":"page"},{"location":"regression/","page":"Regression","title":"Regression","text":"which outputs the following model.","category":"page"},{"location":"regression/","page":"Regression","title":"Regression","text":"(Image: auto partition)","category":"page"},{"location":"regression/","page":"Regression","title":"Regression","text":"The automatic partitioning algorithm has determined that each sub-region benefits from one more split. Currently the automatic partitioning algorithm is only capable of splitting in the center of a sub-region.","category":"page"},{"location":"regression/","page":"Regression","title":"Regression","text":"This simple example is merely to demonstrate the rationale behind these recursively partitioned regression models. The true benefit of these models comes when we move to higher dimensional settings. Using automatic partitioning, we can adapt to the complexity of the data, removing the need to tune the complexity of the model manually. Partitioned models are a type of non-parametric model, meaning they are used when little is known about the structure of the data, and hence a highly flexible model is required.","category":"page"},{"location":"regression/#Functions","page":"Regression","title":"Functions","text":"","category":"section"},{"location":"regression/","page":"Regression","title":"Regression","text":"Modules = [RecursivePartition]\nPages = [\"regression.jl\"]","category":"page"},{"location":"regression/#RecursivePartition.BLMHyper","page":"Regression","title":"RecursivePartition.BLMHyper","text":"BLMHyper(dim::Int64; shape::Float64=0.001, scale::Float64=0.001)\nBLMHyper(coeff::Vector{Float64}, cov::Matrix{Float64},\n    covinv::Matrix{Floa64}, shape::Float64, scale::Float64)\n\nReturn container for the hyperparamters associated with a Bayesian linear model.\n\nIf only the dimension/shape/scale parameters are supplied, the default constructor uses the identity matrix as the covariance matrix, and a vector of zeros for the the prior mean of the model coefficients.\n\nSee also: BayesLinearModel\n\n\n\n\n\n","category":"type"},{"location":"regression/#RecursivePartition.BayesLinearModel","page":"Regression","title":"RecursivePartition.BayesLinearModel","text":"BayesLinearModel(X::Matrix{Float64}, y::Vector{Float64}; shape::Float64=0.001,\n        scale::Float64=0.001)\nBayesLinearModel(X::Matrix{Float64}, y::Vector{Float64}, prior::BLMHyper)\n\nConstruct BayesLinearModel object. One must only supply the dimension of the linear model, and an optional set of prior hyper parameters. Once the model is constructed, fit! can be used to update it with additional data.\n\nThis object implements the well known Bayesian Linear Model with Gaussian responses and unknown variance. A Gaussian/inverse-gamma prior is placed on the model coefficients/variance respectively.\n\nSee also: BLMHyper, fit!\n\nExamples\n\ncoeff = [1.0, 2.0, 3.0]\nf(x) = coeff[1] + coeff[2:3]' * x # Truly linear model\nSD = 1.0\nX, y = gendat(50000, SD, f, 2)\nmodel = BayesLinearModel(X, y)\n\nget_coeffpost(model)\n\n# output\n\n3-element Array{Float64,1}:\n 0.997262172068708\n 1.9957265287418318\n 3.0063668906195202\n\n\n\n\n\n","category":"type"},{"location":"regression/#RecursivePartition.LinearModel","page":"Regression","title":"RecursivePartition.LinearModel","text":"Abstract type encompassing different linear models.\n\nSee also: BayesLinearModel, PolyBLM\n\n\n\n\n\n","category":"type"},{"location":"regression/#RecursivePartition.PartitionHyper","page":"Regression","title":"RecursivePartition.PartitionHyper","text":"PartitionHyper(shape::Float64, scale::Float64,\n        logdet::Union{Nothing, Vector{Float64}})\n\nContainer for hyperparamers which are shared across all subregion models.\n\nShape and scale describe the (prior/posterior) distribution of the unknown noise across the entire space. Each local subregion model will have its own independent shape/scale parameters, which are aggregated with the other subregions to compute the shared hyperparameters.\n\n\n\n\n\n","category":"type"},{"location":"regression/#RecursivePartition.PartitionModel","page":"Regression","title":"RecursivePartition.PartitionModel","text":"PartitionModel{T <: LinearModel}\n\nStruct containing a recursive partition and LinearModel's relating to each subset. Objects of this type are easiest to generate using partition_blm, partition_polyblm.\n\nEach model is assumed to be independent conditional on the unknown noise i.e. the noise is assumed to the constant (but unknown) across the entire space. Intuitively, this means that the models should be adjusted that no one subregion model approximates the noise significantly differently from the rest of the subregions (this would be an indication of under/over-fitting).\n\n\n\n\n\n","category":"type"},{"location":"regression/#RecursivePartition.PolyBLM","page":"Regression","title":"RecursivePartition.PolyBLM","text":"PolyBLM(X, y, degmax, bounds; maxparam=200, shape=0.001, scale0.001,\n    priorgen=identity_hyper)\n\nConstruct a linear model using features derrived from the Polynomial Chaos Basis.\n\nA bound on the maximum number of model parameters, maxparam, is specified by the user. If the number of parameters exceeds this bound, the LARS algorithm is used to choose the \"best\" set of parameters satisfying the bound.\n\nSince we do not know which/how many parameters will be included in the model (we only that the number of parameters is bounded by maxparam), it is not possible to supply an object of type BLMHyper as a prior distribution. Instead, the argument priorgen accepts a function which specifies how the prior distribution should be generated. priorgen must have the signature\n\npriorgen(indices::Vector{MVPIndex}, shape::Float64, scale::Float64)\n\nand return an object of type BLMHyper\n\nSee also: BayesLinearModel, BLMHyper, MVPIndex\n\n\n\n\n\n","category":"type"},{"location":"regression/#RecursivePartition.auto_partition_blm-Tuple{Array{Float64,2},Array{Float64,1},Union{Array{Float64,2}, Array{Float64,1}}}","page":"Regression","title":"RecursivePartition.auto_partition_blm","text":"autopartitionblm(X::Matrix{Float64}, y::Vector{Float64},         bounds::Union{Matrix{Float64}, Vector{Float64}};         mindat=nothing, Kmax=200, shape=0.001, scale=0.001, verbose=false) autopartitionblm(X::Matrix{Float64}, y::Vector{Float64},         bounds::Union{Matrix{Float64}, Vector{Float64}}, prior::BLMHyper;         mindat=nothing, Kmax=200, verbose=false)\n\nAdaptively contruct a partition and fit a PartitionModel, using BayesLinearModel models in each subregion.\n\n\n\n\n\n","category":"method"},{"location":"regression/#RecursivePartition.auto_partition_polyblm-Tuple{Array{Float64,2},Array{Float64,1},Union{Array{Float64,2}, Array{Float64,1}}}","page":"Regression","title":"RecursivePartition.auto_partition_polyblm","text":"auto_partition_polyblm(X::Matrix{Float64}, y::Vector{Float64},\n        bounds::Union{Matrix{Float64}, Vector{Float64}}; degmax::Int64=3,\n        maxparam::Int64=200, priorgen::Function=identity_hyper,\n        shape::Float64=0.001, scale::Float64=0.001, mindat=nothing, Kmax=200,\n        verbose=false)\n\nAdaptively contruct a partition and fit a PartitionModel, using PolyBLM models in each subregion.\n\n\n\n\n\n","category":"method"},{"location":"regression/#RecursivePartition.fit!-Tuple{BayesLinearModel,Array{Float64,2},Array{Float64,1}}","page":"Regression","title":"RecursivePartition.fit!","text":"fit!(mod::BayesLinearModel, X::Matrix{Float64}, y::Vector{Float64})\n\nUpdate a Bayesian Linear Model object with data matrix and response variables.\n\nSee also: BayesLinearModel\n\nExamples\n\ncoeff = [1.0, 2.0, 3.0]\nf(x) = coeff[1] + coeff[2:3]' * x # Truly linear model\nSD = 1.0\nX1, y1 = gendat(25000, SD, f, 2)\nX2, y2 = gendat(25000, SD, f, 2)\nmodel = BayesLinearModel(X1, y1) # Fit initial model\nfit!(model, X2, y2) # Update model with more data\n\nget_coeffpost(model)\n\n# output\n\n3-element Array{Float64,1}:\n 0.996650221433608\n 1.9907052757630952\n 2.9924622031151826\n\n\n\n\n\n","category":"method"},{"location":"regression/#RecursivePartition.logevidence-Tuple{BayesLinearModel}","page":"Regression","title":"RecursivePartition.logevidence","text":"logevidence(mod::LinearModel)\n\nCompute the Bayesian Model Evidence/Marginal Likelihood.\n\n\n\n\n\n","category":"method"},{"location":"regression/#RecursivePartition.partition_blm-Tuple{Array{Float64,2},Array{Float64,1},Array{Array{T,2},1} where T}","page":"Regression","title":"RecursivePartition.partition_blm","text":"partition_blm(X::Matrix{Float64}, y::Vector{Float64},\n        P::Matvec; shape::Float64=0.001, scale::Float64=0.001)\npartition_blm(X::Matrix{Float64}, y::Vector{Float64},\n        P::Matvec, prior::BLMHyper)\n\nConstruct a PartitionModel object, in which the subregion models are comprised of BayesLinearModel models with a fixed (and prespecified) partition.\n\n\n\n\n\n","category":"method"},{"location":"regression/#RecursivePartition.partition_polyblm-Tuple{Array{Float64,2},Array{Float64,1},Array{Array{T,2},1} where T}","page":"Regression","title":"RecursivePartition.partition_polyblm","text":"partition_polyblm(X::Matrix{Float64}, y::Vector{Float64},\n        P::Matvec; degmax::Int64=3, maxparam::Int64=200,\n        priorgen::Function=identity_hyper, shape::Float64=0.001,\n        scale::Float64=0.001)\n\nConstruct a PartitionModel object, in which the subregion models are comprised of PolyBLM models with a fixed (and prespecified) partition.\n\n\n\n\n\n","category":"method"},{"location":"regression/#RecursivePartition.predfun-Tuple{BayesLinearModel}","page":"Regression","title":"RecursivePartition.predfun","text":"predfun(mod::LinearModel)\n\nReturn a function which predicts response variables given an input vector x.\n\n\n\n\n\n","category":"method"},{"location":"regression/#RecursivePartition.predict-Tuple{BayesLinearModel,Array{Float64,2}}","page":"Regression","title":"RecursivePartition.predict","text":"predict(mod::BayesLinearModel, X::Matrix{Float64})\n\nPredict responses based on data matrix X. ```\n\n\n\n\n\n","category":"method"},{"location":"regression/#RecursivePartition.split_subset-Union{Tuple{T}, Tuple{PartitionModel{T},Array{RecursivePartition.SubsetMem{T},1},Int64,Int64,Float64,Int64,ModArgs}} where T<:RecursivePartition.LinearModel","page":"Regression","title":"RecursivePartition.split_subset","text":"split_subset(mod::PartitionModel{T}, stored::Vector{SubsetMem{T}},\n        k::Int64, dim::Int64, loc::Float64, mindat::Int64,\n        args::ModArgs) where T <: LinearModel\n\nReturn a PartitionModel where the k'th subset has been divided in a specficied dimension/location.\n\n\n\n\n\n","category":"method"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = RecursivePartition","category":"page"},{"location":"#RecursivePartition","page":"Home","title":"RecursivePartition","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"","category":"page"}]
}
