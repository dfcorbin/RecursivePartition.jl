<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Regression · RecursivePartition.jl</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">RecursivePartition.jl</span></div><form class="docs-search" action="../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><span class="tocitem">Polynomial Basis Expansion</span><ul><li><a class="tocitem" href="../legendre/">Legendre Polynomials</a></li><li><a class="tocitem" href="../pcb/">Polynomial Chaos Basis</a></li></ul></li><li><a class="tocitem" href="../partition/">Recursive Partitioning</a></li><li class="is-active"><a class="tocitem" href>Regression</a><ul class="internal"><li><a class="tocitem" href="#Bayesian-Linear-Models"><span>Bayesian Linear Models</span></a></li><li><a class="tocitem" href="#Partitioned-Models"><span>Partitioned Models</span></a></li><li><a class="tocitem" href="#Functions"><span>Functions</span></a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Regression</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Regression</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/dfcorbin/RecursivePartition.jl/blob/master/docs/src/regression.md#L" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Regression"><a class="docs-heading-anchor" href="#Regression">Regression</a><a id="Regression-1"></a><a class="docs-heading-anchor-permalink" href="#Regression" title="Permalink"></a></h1><p>The objective of regression is to model the relationship between a set of predictor and response variables. Once trained, the regression model can be used to predict the response variable, given a new predictor variable. RecursivePartition implements a variety of regression models, however its most notable feature is the <a href="#RecursivePartition.PartitionModel"><code>PartitionModel</code></a> type. This model is fitted by partitioning the space of predictor variables (see <a href="../partition/#Recursive-Partitioning">Recursive Partitioning</a> for a visualization of this process) and fitting a flexible model within each subregion. One can think of this as a <em>divide and conquer</em> approach to learning complex relationships between predictors and responses.</p><p>Standard linear models will first be introduced, primarily motivating the use of the <a href="../pcb/#Polynomial-Chaos-Basis">Polynomial Chaos Basis</a> expansion as a method for incorporating a non-linear structure into regression models. The <a href="#Partitioned-Models">Partitioned Models</a> tutorial then shows how we can aggregate the linear models across a partitioned space.</p><p>Note that the examples in this tutorial are at most 2-dimensional for visualization purposes. However, usage is identical for higher dimensional data.</p><h2 id="Bayesian-Linear-Models"><a class="docs-heading-anchor" href="#Bayesian-Linear-Models">Bayesian Linear Models</a><a id="Bayesian-Linear-Models-1"></a><a class="docs-heading-anchor-permalink" href="#Bayesian-Linear-Models" title="Permalink"></a></h2><p>All the Bayesian linear models implemented in RecursivePartition currently assume that the predictor variables are Normally distributed with a mean given by some unknown function i.e.</p><p class="math-container">\[y = f(\boldsymbol{x}) + ϵ, \quad ϵ \sim \mathcal{N}(0, σ^2)\]</p><p>For this tutorial we choose an arbitrarily choose the mean function</p><p class="math-container">\[f(x) = 2 x \text{sin}(2 π x) - 2 x^2\]</p><p>and generate a random data set.</p><pre><code class="language-julia">using RecursivePartition, Plots, Random

Random.seed!(100) # Set seed for reproducability

N = 10000
SD = 0.5
f(x) = 2 * sin(2 * π * x[1]) * x[1] - 2 * x[1]^2
x, y = gendat(N, SD, f, 1; bounds=[-1.0, 1.0])

plot(x, y, seriestype=:scatter, alpha=0.1, label=&quot;data&quot;)
plot!(f, xlims=[-1.0, 1.0], lw=2, lty=2, s=:dash, label=&quot;f(x)&quot;)</code></pre><p>The figure below shows the plot outputted by the above code.</p><p><img src="../figures/fplot.png" alt="f plot"/></p><p>Our job us thus to use the noisy blue data points to approximated the true mean of the data (given by the dashed red line). It is important to remember that the mean function <span>$f$</span> is used only to generate the data; it is unknown to the regression models.</p><p>In this tutorial we will fit three different Bayesian models to the dataset shown in the figure above.</p><ul><li>A linear model.</li><li>An order 5 polynomial regression.</li><li>An order 10 polynomial regression.</li></ul><p>In a polynomial regression we take the original predictor variables and transform them using a polynomial basis (see <a href="../pcb/#Polynomial-Chaos-Basis">Polynomial Chaos Basis</a>). Simply put, we are constructing the &quot;best&quot; polynomial which approximates <span>$f$</span>.</p><pre><code class="language-julia"># Fit a linear model
linearmod = BayesLinearModel(x, y)
linfun = predfun(linearmod)

# For plotting purposes, we need a function which accepts a float, not an
# array...
linfun_flat(x) = linfun([x])

# Fit an order 5 and 10 polynomial model
polymod5 = PolyBLM(x, y, 5, [-1.0, 1.0])
polymod10 = PolyBLM(x, y, 10, [-1.0, 1.0])
polyfun5 = predfun(polymod5)
polyfun10 = predfun(polymod10)
polyfun5_flat(x) = polyfun5([x])
polyfun10_flat(x) = polyfun10([x])

# Create plot
plot(linfun1, xlims=[-1.0, 1.0])
plot!(f, xlims=[-1.0, 1.0], label=&quot;f(x)&quot;, lw=3, s=:dash)
plot!(linfun_flat, xlims=[-1.0, 1.0], label=&quot;Linear mod&quot;)
plot!(polyfun5_flat, xlims=[-1.0, 1.0], label=&quot;Deg 5 poly&quot;)
plot!(polyfun10_flat, xlims=[-1.0, 1.0], label=&quot;Deg 10 poly&quot;)</code></pre><p>In the above code, [<code>predfun</code>] outputs a function which accepts a vector argument. This function takes a predictor and ouputs the predicted response according to the model. The figure below shows the plot outputted using this code.</p><p><img src="../figures/models.png" alt="model plot"/></p><p>Firstly we note that (unsurprisingly) the linear model performs very poorly here. This is because using <a href="#RecursivePartition.BayesLinearModel"><code>BayesLinearModel</code></a> restricts our model to a (linear) straight line, however the structure of the data is clearly non-linear. The order 5 polynomial model does a reasonable job of approximating <span>$f$</span>, however our model is still not flexible enough. Once we increase the order (and hence the complexity of the model) to 10, we finally have a model which accurately approximates <span>$f$</span>.</p><p>With the model trained, we can predict response variables as follows.</p><pre><code class="language-julia">Xtest, ytest = gendat(N, SD, f, 1; bounds=[-1.0, 1.0])
ytest_approx = predict(polymod10, Xtest)</code></pre><h2 id="Partitioned-Models"><a class="docs-heading-anchor" href="#Partitioned-Models">Partitioned Models</a><a id="Partitioned-Models-1"></a><a class="docs-heading-anchor-permalink" href="#Partitioned-Models" title="Permalink"></a></h2><p>Although the polynomial regression models above may seem appealing, they quickly become impractical as the dimension of the predictor variables (and the degree of the model) increases. As an alternative approach, we can partition the space into separate regions and fit low order polynomial model within each one. For example, the following code will divide the data in half at 0, then fit an order 3 polynomial regression inside each half.</p><pre><code class="language-julia"># Construct partition
P = [[-1.0 1.0]] # Define the whole space (x lies between -1 and 1).
insert_knot!(P, 1, 1, 0.0) # Split in the middle.

# Fit partitioned polynomial models
partition_model = partition_polyblm(x, y, P; degmax=3)</code></pre><p>We no longer provide code for producing figures, as the process is essentially the same. The figure below should the partitioned approximation</p><p><img src="../figures/part.png" alt="Static partition"/></p><p>This approximation isn&#39;t terrible, but the natural response is to wonder whether we should partition the space further. In higher dimensional settings we don&#39;t have the luxury of plotting the function and choosing a sensible partition. Luckily, RecursivePartition implements a novel algorithm for dividing up the space automatically.</p><pre><code class="language-julia">automod = auto_partition_polyblm(x, y, [-1.0, 1.0])</code></pre><p>which outputs the following model.</p><p><img src="../figures/autopart.png" alt="auto partition"/></p><p>The automatic partitioning algorithm has determined that each sub-region benefits from one more split. Currently the automatic partitioning algorithm is only capable of splitting in the center of a sub-region.</p><p>This simple example is merely to demonstrate the rationale behind these recursively partitioned regression models. The true benefit of these models comes when we move to higher dimensional settings. Using automatic partitioning, we can adapt to the complexity of the data, removing the need to tune the complexity of the model manually. Partitioned models are a type of non-parametric model, meaning they are used when little is known about the structure of the data, and hence a highly flexible model is required.</p><h2 id="Functions"><a class="docs-heading-anchor" href="#Functions">Functions</a><a id="Functions-1"></a><a class="docs-heading-anchor-permalink" href="#Functions" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="RecursivePartition.BLMHyper" href="#RecursivePartition.BLMHyper"><code>RecursivePartition.BLMHyper</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">BLMHyper(dim::Int64; shape::Float64=0.001, scale::Float64=0.001)
BLMHyper(coeff::Vector{Float64}, cov::Matrix{Float64},
    covinv::Matrix{Floa64}, shape::Float64, scale::Float64)</code></pre><p>Return container for the hyperparamters associated with a Bayesian linear model.</p><p>If only the dimension/shape/scale parameters are supplied, the default constructor uses the identity matrix as the covariance matrix, and a vector of zeros for the the prior mean of the model coefficients.</p><p>See also: <a href="#RecursivePartition.BayesLinearModel"><code>BayesLinearModel</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dfcorbin/RecursivePartition.jl/blob/9bad1ff5b814b8fd6f65697d24ae984599c3ce90/src/regression.jl#LL1-L14">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="RecursivePartition.BayesLinearModel" href="#RecursivePartition.BayesLinearModel"><code>RecursivePartition.BayesLinearModel</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">BayesLinearModel(X::Matrix{Float64}, y::Vector{Float64}; shape::Float64=0.001,
        scale::Float64=0.001)
BayesLinearModel(X::Matrix{Float64}, y::Vector{Float64}, prior::BLMHyper)</code></pre><p>Construct <a href="#RecursivePartition.BayesLinearModel"><code>BayesLinearModel</code></a> object. One must only supply the dimension of the linear model, and an optional set of prior hyper parameters. Once the model is constructed, <a href="#RecursivePartition.fit!-Tuple{BayesLinearModel, Matrix{Float64}, Vector{Float64}}"><code>fit!</code></a> can be used to update it with additional data.</p><p>This object implements the well known Bayesian Linear Model with Gaussian responses and unknown variance. A Gaussian/inverse-gamma prior is placed on the model coefficients/variance respectively.</p><p>See also: <a href="#RecursivePartition.BLMHyper"><code>BLMHyper</code></a>, <a href="#RecursivePartition.fit!-Tuple{BayesLinearModel, Matrix{Float64}, Vector{Float64}}"><code>fit!</code></a></p><p><strong>Examples</strong></p><pre><code class="language-julia">coeff = [1.0, 2.0, 3.0]
f(x) = coeff[1] + coeff[2:3]&#39; * x # Truly linear model
SD = 1.0
X, y = gendat(50000, SD, f, 2)
model = BayesLinearModel(X, y)

get_coeffpost(model)

# output

3-element Array{Float64,1}:
 0.997262172068708
 1.9957265287418318
 3.0063668906195202</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dfcorbin/RecursivePartition.jl/blob/9bad1ff5b814b8fd6f65697d24ae984599c3ce90/src/regression.jl#LL48-L82">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="RecursivePartition.LinearModel" href="#RecursivePartition.LinearModel"><code>RecursivePartition.LinearModel</code></a> — <span class="docstring-category">Type</span></header><section><div><p>Abstract type encompassing different linear models.</p><p>See also: <a href="#RecursivePartition.BayesLinearModel"><code>BayesLinearModel</code></a>, <a href="#RecursivePartition.PolyBLM"><code>PolyBLM</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dfcorbin/RecursivePartition.jl/blob/9bad1ff5b814b8fd6f65697d24ae984599c3ce90/src/regression.jl#LL40-L44">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="RecursivePartition.PartitionHyper" href="#RecursivePartition.PartitionHyper"><code>RecursivePartition.PartitionHyper</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">PartitionHyper(shape::Float64, scale::Float64,
        logdet::Union{Nothing, Vector{Float64}})</code></pre><p>Container for hyperparamers which are shared across all subregion models.</p><p>Shape and scale describe the (prior/posterior) distribution of the unknown noise across the entire space. Each local subregion model will have its own independent shape/scale parameters, which are aggregated with the other subregions to compute the shared hyperparameters.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dfcorbin/RecursivePartition.jl/blob/9bad1ff5b814b8fd6f65697d24ae984599c3ce90/src/partition_regression.jl#LL7-L17">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="RecursivePartition.PartitionModel" href="#RecursivePartition.PartitionModel"><code>RecursivePartition.PartitionModel</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">PartitionModel{T &lt;: LinearModel}</code></pre><p>Struct containing a recursive partition and <a href="#RecursivePartition.LinearModel"><code>LinearModel</code></a>&#39;s relating to each subset. Objects of this type are easiest to generate using <a href="#RecursivePartition.partition_blm-Tuple{Matrix{Float64}, Vector{Float64}, Array{Matrix{T}, 1} where T}"><code>partition_blm</code></a>, <a href="#RecursivePartition.partition_polyblm-Tuple{Matrix{Float64}, Vector{Float64}, Array{Matrix{T}, 1} where T}"><code>partition_polyblm</code></a>.</p><p>Each model is assumed to be independent conditional on the unknown noise i.e. the noise is assumed to the constant (but unknown) across the entire space. Intuitively, this means that the models should be adjusted that no one subregion model approximates the noise significantly differently from the rest of the subregions (this would be an indication of under/over-fitting).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dfcorbin/RecursivePartition.jl/blob/9bad1ff5b814b8fd6f65697d24ae984599c3ce90/src/partition_regression.jl#LL25-L37">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="RecursivePartition.PolyBLM" href="#RecursivePartition.PolyBLM"><code>RecursivePartition.PolyBLM</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">PolyBLM(X, y, degmax, bounds; maxparam=200, shape=0.001, scale0.001,
    priorgen=identity_hyper)</code></pre><p>Construct a linear model using features derrived from the <a href="../pcb/#Polynomial-Chaos-Basis">Polynomial Chaos Basis</a>.</p><p>A bound on the maximum number of model parameters, <code>maxparam</code>, is specified by the user. If the number of parameters exceeds this bound, the LARS algorithm is used to choose the &quot;best&quot; set of parameters satisfying the bound.</p><p>Since we do not know which/how many parameters will be included in the model (we only that the number of parameters is bounded by <code>maxparam</code>), it is not possible to supply an object of type <a href="#RecursivePartition.BLMHyper"><code>BLMHyper</code></a> as a prior distribution. Instead, the argument <code>priorgen</code> accepts a function which specifies how the prior distribution should be generated. <code>priorgen</code> <strong>must</strong> have the signature</p><pre><code class="language-none">priorgen(indices::Vector{MVPIndex}, shape::Float64, scale::Float64)</code></pre><p>and return an object of type <a href="#RecursivePartition.BLMHyper"><code>BLMHyper</code></a></p><p>See also: <a href="#RecursivePartition.BayesLinearModel"><code>BayesLinearModel</code></a>, <a href="#RecursivePartition.BLMHyper"><code>BLMHyper</code></a>, <a href="../pcb/#RecursivePartition.MVPIndex"><code>MVPIndex</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dfcorbin/RecursivePartition.jl/blob/9bad1ff5b814b8fd6f65697d24ae984599c3ce90/src/regression.jl#LL281-L303">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="RecursivePartition.auto_partition_blm-Tuple{Matrix{Float64}, Vector{Float64}, VecOrMat{Float64}}" href="#RecursivePartition.auto_partition_blm-Tuple{Matrix{Float64}, Vector{Float64}, VecOrMat{Float64}}"><code>RecursivePartition.auto_partition_blm</code></a> — <span class="docstring-category">Method</span></header><section><div><p>auto<em>partition</em>blm(X::Matrix{Float64}, y::Vector{Float64},         bounds::Union{Matrix{Float64}, Vector{Float64}};         mindat=nothing, Kmax=200, shape=0.001, scale=0.001, verbose=false) auto<em>partition</em>blm(X::Matrix{Float64}, y::Vector{Float64},         bounds::Union{Matrix{Float64}, Vector{Float64}}, prior::BLMHyper;         mindat=nothing, Kmax=200, verbose=false)</p><p>Adaptively contruct a partition and fit a <a href="#RecursivePartition.PartitionModel"><code>PartitionModel</code></a>, using <a href="#RecursivePartition.BayesLinearModel"><code>BayesLinearModel</code></a> models in each subregion.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dfcorbin/RecursivePartition.jl/blob/9bad1ff5b814b8fd6f65697d24ae984599c3ce90/src/partition_regression.jl#LL653-L663">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="RecursivePartition.auto_partition_polyblm-Tuple{Matrix{Float64}, Vector{Float64}, VecOrMat{Float64}}" href="#RecursivePartition.auto_partition_polyblm-Tuple{Matrix{Float64}, Vector{Float64}, VecOrMat{Float64}}"><code>RecursivePartition.auto_partition_polyblm</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">auto_partition_polyblm(X::Matrix{Float64}, y::Vector{Float64},
        bounds::Union{Matrix{Float64}, Vector{Float64}}; degmax::Int64=3,
        maxparam::Int64=200, priorgen::Function=identity_hyper,
        shape::Float64=0.001, scale::Float64=0.001, mindat=nothing, Kmax=200,
        verbose=false)</code></pre><p>Adaptively contruct a partition and fit a <a href="#RecursivePartition.PartitionModel"><code>PartitionModel</code></a>, using <a href="#RecursivePartition.PolyBLM"><code>PolyBLM</code></a> models in each subregion.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dfcorbin/RecursivePartition.jl/blob/9bad1ff5b814b8fd6f65697d24ae984599c3ce90/src/partition_regression.jl#LL582-L591">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="RecursivePartition.fit!-Tuple{BayesLinearModel, Matrix{Float64}, Vector{Float64}}" href="#RecursivePartition.fit!-Tuple{BayesLinearModel, Matrix{Float64}, Vector{Float64}}"><code>RecursivePartition.fit!</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">fit!(mod::BayesLinearModel, X::Matrix{Float64}, y::Vector{Float64})</code></pre><p>Update a Bayesian Linear Model object with data matrix and response variables.</p><p>See also: <a href="#RecursivePartition.BayesLinearModel"><code>BayesLinearModel</code></a></p><p><strong>Examples</strong></p><pre><code class="language-julia">coeff = [1.0, 2.0, 3.0]
f(x) = coeff[1] + coeff[2:3]&#39; * x # Truly linear model
SD = 1.0
X1, y1 = gendat(25000, SD, f, 2)
X2, y2 = gendat(25000, SD, f, 2)
model = BayesLinearModel(X1, y1) # Fit initial model
fit!(model, X2, y2) # Update model with more data

get_coeffpost(model)

# output

3-element Array{Float64,1}:
 0.996650221433608
 1.9907052757630952
 2.9924622031151826</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dfcorbin/RecursivePartition.jl/blob/9bad1ff5b814b8fd6f65697d24ae984599c3ce90/src/regression.jl#LL152-L179">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="RecursivePartition.logevidence-Tuple{BayesLinearModel}" href="#RecursivePartition.logevidence-Tuple{BayesLinearModel}"><code>RecursivePartition.logevidence</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">logevidence(mod::LinearModel)</code></pre><p>Compute the Bayesian Model Evidence/Marginal Likelihood.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dfcorbin/RecursivePartition.jl/blob/9bad1ff5b814b8fd6f65697d24ae984599c3ce90/src/regression.jl#LL260-L264">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="RecursivePartition.partition_blm-Tuple{Matrix{Float64}, Vector{Float64}, Array{Matrix{T}, 1} where T}" href="#RecursivePartition.partition_blm-Tuple{Matrix{Float64}, Vector{Float64}, Array{Matrix{T}, 1} where T}"><code>RecursivePartition.partition_blm</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">partition_blm(X::Matrix{Float64}, y::Vector{Float64},
        P::Matvec; shape::Float64=0.001, scale::Float64=0.001)
partition_blm(X::Matrix{Float64}, y::Vector{Float64},
        P::Matvec, prior::BLMHyper)</code></pre><p>Construct a <a href="#RecursivePartition.PartitionModel"><code>PartitionModel</code></a> object, in which the subregion models are comprised of <a href="#RecursivePartition.BayesLinearModel"><code>BayesLinearModel</code></a> models with a fixed (and prespecified) partition.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dfcorbin/RecursivePartition.jl/blob/9bad1ff5b814b8fd6f65697d24ae984599c3ce90/src/partition_regression.jl#LL205-L214">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="RecursivePartition.partition_polyblm-Tuple{Matrix{Float64}, Vector{Float64}, Array{Matrix{T}, 1} where T}" href="#RecursivePartition.partition_polyblm-Tuple{Matrix{Float64}, Vector{Float64}, Array{Matrix{T}, 1} where T}"><code>RecursivePartition.partition_polyblm</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">partition_polyblm(X::Matrix{Float64}, y::Vector{Float64},
        P::Matvec; degmax::Int64=3, maxparam::Int64=200,
        priorgen::Function=identity_hyper, shape::Float64=0.001,
        scale::Float64=0.001)</code></pre><p>Construct a <a href="#RecursivePartition.PartitionModel"><code>PartitionModel</code></a> object, in which the subregion models are comprised of <a href="#RecursivePartition.PolyBLM"><code>PolyBLM</code></a> models with a fixed (and prespecified) partition.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dfcorbin/RecursivePartition.jl/blob/9bad1ff5b814b8fd6f65697d24ae984599c3ce90/src/partition_regression.jl#LL174-L183">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="RecursivePartition.predfun-Tuple{BayesLinearModel}" href="#RecursivePartition.predfun-Tuple{BayesLinearModel}"><code>RecursivePartition.predfun</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">predfun(mod::LinearModel)</code></pre><p>Return a function which predicts response variables given an input vector <code>x</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dfcorbin/RecursivePartition.jl/blob/9bad1ff5b814b8fd6f65697d24ae984599c3ce90/src/regression.jl#LL245-L250">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="RecursivePartition.predict-Tuple{BayesLinearModel, Matrix{Float64}}" href="#RecursivePartition.predict-Tuple{BayesLinearModel, Matrix{Float64}}"><code>RecursivePartition.predict</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">predict(mod::BayesLinearModel, X::Matrix{Float64})</code></pre><p>Predict responses based on data matrix <code>X</code>. ```</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dfcorbin/RecursivePartition.jl/blob/9bad1ff5b814b8fd6f65697d24ae984599c3ce90/src/regression.jl#LL234-L239">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="RecursivePartition.split_subset-Union{Tuple{T}, Tuple{PartitionModel{T}, Array{RecursivePartition.SubsetMem{T}, 1}, Int64, Int64, Float64, Int64, ModArgs}} where T&lt;:RecursivePartition.LinearModel" href="#RecursivePartition.split_subset-Union{Tuple{T}, Tuple{PartitionModel{T}, Array{RecursivePartition.SubsetMem{T}, 1}, Int64, Int64, Float64, Int64, ModArgs}} where T&lt;:RecursivePartition.LinearModel"><code>RecursivePartition.split_subset</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">split_subset(mod::PartitionModel{T}, stored::Vector{SubsetMem{T}},
        k::Int64, dim::Int64, loc::Float64, mindat::Int64,
        args::ModArgs) where T &lt;: LinearModel</code></pre><p>Return a <a href="#RecursivePartition.PartitionModel"><code>PartitionModel</code></a> where the <code>k</code>&#39;th subset has been divided in a specficied dimension/location.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/dfcorbin/RecursivePartition.jl/blob/9bad1ff5b814b8fd6f65697d24ae984599c3ce90/src/partition_regression.jl#LL345-L352">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../partition/">« Recursive Partitioning</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Friday 14 May 2021 11:18">Friday 14 May 2021</span>. Using Julia version 1.6.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
